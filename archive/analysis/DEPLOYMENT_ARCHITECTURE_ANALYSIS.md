# Filter-iCal SSM Deployment Architecture Analysis

**Date:** 2025-10-11
**Analyzed by:** Claude Code Agent
**Scope:** Complete end-to-end SSM-based deployment system

---

## Executive Summary

The filter-ical deployment uses **AWS Systems Manager (SSM)** for serverless deployment orchestration, eliminating the need for SSH keys while maintaining full CI/CD capabilities. The architecture achieves **5-minute deployments** with **zero external dependencies** (no container registry) by building Docker images directly on the EC2 instance.

**Key Strengths:**
- No SSH keys or secrets management (SSM handles authentication)
- Automatic health checks prevent broken deployments from going live
- Environment isolation via Docker Compose project names
- Complete traceability via AWS SSM command history

**Critical Weaknesses:**
- **5-minute build time bottleneck** (builds on server, not in CI)
- **Zero-downtime deployments not possible** (requires blue-green or rolling updates)
- **No rollback capability** (git reset is destructive)
- **Single point of failure** (EC2 instance)

**Top Priority Improvements:**
1. Implement container registry (GHCR) to move builds to CI (30s deploys instead of 5min)
2. Add blue-green deployment for zero-downtime updates
3. Implement proper rollback mechanism (tag-based or container versioning)

---

## 1. Architecture Overview

### 1.1 System Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Developer Machine                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Local      ‚îÇ    ‚îÇ   Git Repo   ‚îÇ    ‚îÇ     AWS      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Makefile   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (GitHub)   ‚îÇ    ‚îÇ     CLI      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                   ‚îÇ           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ   AWS Systems Manager (SSM)   ‚îÇ
                                    ‚îÇ   - No SSH keys needed        ‚îÇ
                                    ‚îÇ   - IAM-based authentication  ‚îÇ
                                    ‚îÇ   - Command history tracking  ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ   EC2 Instance (eu-north-1)  ‚îÇ
                                    ‚îÇ   i-01647c3d9af4fe9fc         ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ                           ‚îÇ                           ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  /opt/apps/          ‚îÇ    ‚îÇ  /opt/platform/    ‚îÇ    ‚îÇ   Docker Network     ‚îÇ
            ‚îÇ  filter-ical/        ‚îÇ    ‚îÇ  (nginx, postgres) ‚îÇ    ‚îÇ   "platform"         ‚îÇ
            ‚îÇ                      ‚îÇ    ‚îÇ                    ‚îÇ    ‚îÇ                      ‚îÇ
            ‚îÇ  ‚îú‚îÄ‚îÄ backend/        ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ nginx/        ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ backend:3000   ‚îÇ
            ‚îÇ  ‚îú‚îÄ‚îÄ frontend/       ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ postgres/     ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ frontend:80    ‚îÇ
            ‚îÇ  ‚îú‚îÄ‚îÄ deploy.sh       ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ certbot/      ‚îÇ    ‚îÇ  ‚îú‚îÄ‚îÄ nginx-platform ‚îÇ
            ‚îÇ  ‚îú‚îÄ‚îÄ docker-compose  ‚îÇ    ‚îÇ                    ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ postgres-...   ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ .env.{env}      ‚îÇ    ‚îÇ                    ‚îÇ    ‚îÇ                      ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Deployment Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DEPLOYMENT FLOW: Local ‚Üí GitHub ‚Üí SSM ‚Üí EC2 ‚Üí Docker                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

STEP 1: LOCAL TESTING
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Developer    ‚îÇ
‚îÇ makes change ‚îÇ‚îÄ‚îÄ‚ñ∂ git commit
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ (optional: git push for backup)
       ‚îÇ
       ‚ñº

STEP 2: DEPLOYMENT TRIGGER
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ make deploy-     ‚îÇ
‚îÇ staging          ‚îÇ‚îÄ‚îÄ‚ñ∂ Runs tests first (make test)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
       ‚îÇ                       ‚îÇ (can skip: SKIP_TESTS=1)
       ‚îÇ                       ‚îÇ
       ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tests pass?      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Tests fail? ‚îÇ‚îÄ‚îÄ‚ñ∂ ABORT (no deploy)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ YES
       ‚ñº

STEP 3: SSM COMMAND DISPATCH
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ aws ssm send-command                 ‚îÇ
‚îÇ --instance-ids i-01647c3d9af4fe9fc   ‚îÇ
‚îÇ --document-name AWS-RunShellScript   ‚îÇ
‚îÇ --parameters "commands=[...]"        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ (SSM authenticates via IAM role)
       ‚îÇ
       ‚ñº

STEP 4: EC2 EXECUTION
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ cd /opt/apps/filter-ical             ‚îÇ
‚îÇ git fetch origin                     ‚îÇ
‚îÇ git reset --hard origin/main         ‚îÇ  ‚ö†Ô∏è DESTRUCTIVE (no rollback)
‚îÇ export ENVIRONMENT=staging           ‚îÇ
‚îÇ docker-compose -p filter-ical-       ‚îÇ
‚îÇ   staging up -d --build              ‚îÇ  ‚ö†Ô∏è SLOW (5 min build)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ (waits for containers to start)
       ‚îÇ
       ‚ñº

STEP 5: DATABASE MIGRATIONS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ sleep 10                             ‚îÇ  ‚ö†Ô∏è ARBITRARY WAIT
‚îÇ docker-compose exec -T backend       ‚îÇ
‚îÇ   alembic upgrade head               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº

STEP 6: HEALTH CHECK
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ curl -f https://staging.             ‚îÇ
‚îÇ   filter-ical.de/health              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ‚ñ∂ SUCCESS: Deployment complete ‚úÖ
       ‚îÇ
       ‚îî‚îÄ‚ñ∂ FAIL: Instructions to check logs ‚ùå
           (make logs-staging)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RESULT: New containers running on EC2, accessible via nginx reverse proxy   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. Component Breakdown

### 2.1 Local Development Machine

**Files:**
- `/Users/martijn/Documents/Projects/filter-ical/deploy.sh` - SSM deployment orchestrator
- `/Users/martijn/Documents/Projects/filter-ical/Makefile` - Developer interface
- `/Users/martijn/Documents/Projects/filter-ical/.env.ec2` - EC2 instance ID (gitignored)

**Responsibilities:**
- Run unit tests before deployment (`make test`)
- Send SSM commands to EC2 instance
- Wait for deployment completion
- Verify health check passes

**Key Design Decisions:**
- No SSH keys required (SSM handles auth via IAM)
- Environment-specific targets (`make deploy-staging` vs `make deploy-production`)
- Optional test skipping (`SKIP_TESTS=1 make deploy-staging` for emergencies)

### 2.2 AWS Systems Manager (SSM)

**Purpose:** Serverless deployment orchestration without SSH

**Benefits:**
- No SSH key management (uses IAM role authentication)
- Complete command history in AWS CloudWatch
- Built-in `aws ssm wait command-executed` (no arbitrary timeouts)
- Secure session management for debugging (`make logs-staging`)

**How It Works:**
```bash
COMMAND_ID=$(aws ssm send-command \
    --region eu-north-1 \
    --instance-ids i-01647c3d9af4fe9fc \
    --document-name "AWS-RunShellScript" \
    --parameters "commands=[...]" \
    --output text \
    --query 'Command.CommandId')

# Wait for completion (no timeout needed)
aws ssm wait command-executed \
    --command-id $COMMAND_ID \
    --instance-id i-01647c3d9af4fe9fc
```

**Instance Configuration:**
- EC2 instance has `AmazonSSMManagedInstanceCore` IAM policy
- SSM agent runs automatically on Amazon Linux 2023
- No inbound SSH ports required (SSM uses outbound HTTPS)

### 2.3 EC2 Instance

**Details:**
- Instance ID: `i-01647c3d9af4fe9fc`
- Region: `eu-north-1` (Stockholm)
- Public IP: `13.62.136.72`
- OS: Amazon Linux 2023

**Directory Structure:**
```
/opt/
‚îú‚îÄ‚îÄ platform/                  # Multi-tenant infrastructure repo
‚îÇ   ‚îú‚îÄ‚îÄ nginx/                 # Shared reverse proxy
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sites/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ filter-ical.conf
‚îÇ   ‚îú‚îÄ‚îÄ postgres/              # Shared database
‚îÇ   ‚îî‚îÄ‚îÄ certbot/               # SSL certificate manager
‚îÇ
‚îî‚îÄ‚îÄ apps/
    ‚îî‚îÄ‚îÄ filter-ical/           # This application
        ‚îú‚îÄ‚îÄ backend/           # Python FastAPI
        ‚îú‚îÄ‚îÄ frontend/          # Vue 3 SPA
        ‚îú‚îÄ‚îÄ deploy.sh          # Deployment script
        ‚îú‚îÄ‚îÄ docker-compose.yml # Container orchestration
        ‚îú‚îÄ‚îÄ .env.staging       # Staging environment secrets
        ‚îî‚îÄ‚îÄ .env.production    # Production environment secrets
```

### 2.4 Docker Compose Project Isolation

**Environment Isolation via Project Names:**

```yaml
# docker-compose.yml
services:
  backend:
    build: ./backend
    env_file: .env.${ENVIRONMENT}  # Loads .env.staging or .env.production
    networks:
      - platform
    restart: unless-stopped

  frontend:
    build: ./frontend
    networks:
      - platform
    restart: unless-stopped
    depends_on:
      backend:
        condition: service_healthy  # Waits for backend /health endpoint

networks:
  platform:
    external: true  # Joins multi-tenant network for nginx routing
```

**Project Name Determines Container Names:**
```bash
# Staging deployment
ENVIRONMENT=staging
docker-compose -p filter-ical-staging up -d --build
# Creates: filter-ical-staging_backend_1, filter-ical-staging_frontend_1

# Production deployment
ENVIRONMENT=production
docker-compose -p filter-ical-production up -d --build
# Creates: filter-ical-production_backend_1, filter-ical-production_frontend_1
```

**Nginx Routes to Correct Containers:**
```nginx
# Staging
server {
    server_name staging.filter-ical.de;
    location ~ ^/(api|domains|filter|subscribe|ical) {
        proxy_pass http://filter-ical-backend-staging:3000;
    }
    location / {
        proxy_pass http://filter-ical-frontend-staging:80;
    }
}

# Production
server {
    server_name filter-ical.de www.filter-ical.de;
    location ~ ^/(api|domains|filter|subscribe|ical) {
        proxy_pass http://filter-ical-backend-production:3000;
    }
    location / {
        proxy_pass http://filter-ical-frontend-production:80;
    }
}
```

### 2.5 Docker Networking

**Platform Network Architecture:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Docker Network: "platform"                ‚îÇ
‚îÇ                     (Created by multi-tenant platform)           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                   ‚îÇ
‚îÇ  nginx-platform:443 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ       ‚îÇ                            ‚îÇ                             ‚îÇ
‚îÇ       ‚îÇ (reverse proxy)            ‚îÇ (Docker DNS resolution)     ‚îÇ
‚îÇ       ‚îÇ                            ‚îÇ                             ‚îÇ
‚îÇ       ‚îú‚îÄ‚ñ∂ filter-ical-backend-staging:3000                      ‚îÇ
‚îÇ       ‚îú‚îÄ‚ñ∂ filter-ical-backend-production:3000                   ‚îÇ
‚îÇ       ‚îú‚îÄ‚ñ∂ filter-ical-frontend-staging:80                       ‚îÇ
‚îÇ       ‚îî‚îÄ‚ñ∂ filter-ical-frontend-production:80                    ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  postgres-platform:5432                                          ‚îÇ
‚îÇ       ‚îÇ                                                           ‚îÇ
‚îÇ       ‚îú‚îÄ‚ñ∂ filterical_staging (database)                         ‚îÇ
‚îÇ       ‚îî‚îÄ‚ñ∂ filterical_production (database)                      ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Benefits:**
- Container names act as DNS hostnames (Docker's built-in DNS)
- No port mapping needed (containers communicate directly)
- Network isolation from host and other networks
- Nginx can resolve container names at request time (not at startup)

**DNS Resolution Strategy:**
```nginx
# nginx.conf
resolver 127.0.0.11 valid=10s;  # Docker's embedded DNS server

# Dynamic resolution (defers to request time)
set $backend filter-ical-backend-production:3000;
proxy_pass http://$backend;
```

This allows containers to restart without breaking nginx (DNS is re-resolved every 10s).

---

## 3. Deployment Flow Deep Dive

### 3.1 Step-by-Step Execution

**Step 1: Local Testing (Optional but Recommended)**
```bash
make test  # Runs unit tests in backend/venv
```

**Step 2: Trigger Deployment**
```bash
make deploy-staging
# Internally calls: make test && ./deploy.sh staging
```

**Step 3: Deploy Script Sends SSM Command**
```bash
#!/bin/bash
set -e  # Exit on any error

ENVIRONMENT="staging"
INSTANCE_ID="i-01647c3d9af4fe9fc"
REGION="eu-north-1"

# Send command to EC2 via SSM
COMMAND_ID=$(aws ssm send-command \
    --region "$REGION" \
    --instance-ids "$INSTANCE_ID" \
    --document-name "AWS-RunShellScript" \
    --comment "Deploy filter-ical staging" \
    --parameters "commands=[
        'set -e',
        'PROJECT_NAME=filter-ical-staging',
        'cd /opt/apps/filter-ical || exit 1',

        # Clone if first deploy
        'if [ ! -d .git ]; then',
        '  cd /opt/apps',
        '  git clone https://github.com/duersjefen/filter-ical.git',
        '  cd filter-ical',
        'fi',

        # Pull latest code
        'git fetch origin',
        'git reset --hard origin/main',  # ‚ö†Ô∏è DESTRUCTIVE

        # Build and start containers
        'export ENVIRONMENT=staging',
        'export DOCKER_BUILDKIT=1',
        'docker-compose -p \$PROJECT_NAME up -d --build',  # ‚ö†Ô∏è SLOW

        # Wait for backend to start
        'echo \"‚è≥ Waiting for backend...\"',
        'sleep 10',  # ‚ö†Ô∏è ARBITRARY

        # Run migrations
        'echo \"üìä Running migrations...\"',
        'docker-compose -p \$PROJECT_NAME exec -T backend alembic upgrade head',

        'echo \"‚úÖ staging is live!\"'
    ]" \
    --output text \
    --query 'Command.CommandId')

# Wait for completion (uses AWS API, not arbitrary timeout)
aws ssm wait command-executed \
    --region "$REGION" \
    --command-id "$COMMAND_ID" \
    --instance-id "$INSTANCE_ID"
```

**Step 4: Health Check**
```bash
# Verify deployment succeeded
if curl -f -s "https://staging.filter-ical.de/health" > /dev/null 2>&1; then
    echo "‚úÖ Deployment successful!"
else
    echo "‚ùå Health check failed"
    echo "üîç Debug with: make logs-staging"
    exit 1
fi
```

### 3.2 Build Process Analysis

**Current Build Strategy: On-Server Builds**

```dockerfile
# backend/Dockerfile
FROM python:3.11-slim AS base
WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt  # ~60s

# Copy application code
COPY app/ ./app/
COPY openapi.yaml domains/ alembic/ alembic.ini ./

# Test stage (skipped in production)
FROM base AS test
COPY tests/ ./tests/
RUN python3 -m pytest tests/ -m unit -v

# Production stage
FROM base AS production
HEALTHCHECK CMD curl -f http://localhost:3000/health || exit 1
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "3000"]
```

```dockerfile
# frontend/Dockerfile
FROM node:22-alpine AS build
WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm ci  # ~90s

# Build Vue 3 SPA
COPY . .
RUN npm run build  # ~120s (Vite + Tailwind)

# Production stage (nginx with brotli)
FROM fholzer/nginx-brotli:v1.28.0
RUN apk add --no-cache curl
COPY nginx.conf /etc/nginx/nginx.conf
COPY --from=build /app/dist /usr/share/nginx/html
HEALTHCHECK CMD curl -f http://localhost/health || exit 1
```

**Build Time Breakdown:**
```
Backend Build:
‚îú‚îÄ‚îÄ Base image pull: ~10s (cached after first build)
‚îú‚îÄ‚îÄ pip install: ~60s
‚îú‚îÄ‚îÄ Copy code: ~1s
‚îî‚îÄ‚îÄ Total: ~70s

Frontend Build:
‚îú‚îÄ‚îÄ Node image pull: ~5s (cached)
‚îú‚îÄ‚îÄ npm ci: ~90s
‚îú‚îÄ‚îÄ npm run build: ~120s
‚îú‚îÄ‚îÄ nginx image pull: ~3s (cached)
‚îî‚îÄ‚îÄ Total: ~220s

Database Migrations:
‚îî‚îÄ‚îÄ alembic upgrade head: ~5s

Total Deployment Time: ~5 minutes
```

**Why So Slow?**
1. Builds run on EC2 instance (not in CI pipeline)
2. No layer caching between deployments (Docker Buildkit helps but limited)
3. No pre-built images from registry (builds from scratch every time)
4. Frontend build includes Vite + Tailwind compilation (CPU-intensive)

### 3.3 Database Migration Strategy

**Alembic Auto-Migration:**
```bash
# Runs after containers start
docker-compose -p filter-ical-staging exec -T backend alembic upgrade head
```

**Migration Files Location:**
```
backend/
‚îú‚îÄ‚îÄ alembic/
‚îÇ   ‚îú‚îÄ‚îÄ versions/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 001_initial_schema.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 002_add_user_preferences.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ env.py
‚îî‚îÄ‚îÄ alembic.ini
```

**Safety Mechanisms:**
- Alembic tracks applied migrations in `alembic_version` table
- `upgrade head` is idempotent (safe to run multiple times)
- Migrations run inside container (no direct database access needed)

**Risks:**
- Migrations run AFTER containers start (no pre-flight validation)
- Failed migrations leave containers running but database inconsistent
- No automatic rollback if migration fails

---

## 4. Strengths

### 4.1 Security & Access Control

**No SSH Keys Required:**
- SSM uses IAM role authentication (no key management)
- No exposed SSH ports (SSM uses outbound HTTPS only)
- Automatic session logging in CloudWatch
- Granular permissions via IAM policies

**Secrets Management:**
```bash
# Secrets stored on EC2 only (not in git)
/opt/apps/filter-ical/.env.staging
/opt/apps/filter-ical/.env.production

# Local machine only stores instance ID
~/Documents/Projects/filter-ical/.env.ec2
```

### 4.2 Environment Isolation

**Complete Staging/Production Separation:**
- Separate Docker Compose projects (`filter-ical-staging` vs `filter-ical-production`)
- Separate container instances
- Separate databases (`filterical_staging` vs `filterical_production`)
- Separate domains (`staging.filter-ical.de` vs `filter-ical.de`)
- Separate SSL certificates (via SNI routing)

**Nginx Routing Ensures Isolation:**
```nginx
# Staging requests NEVER reach production containers
server {
    server_name staging.filter-ical.de;
    proxy_pass http://filter-ical-backend-staging:3000;  # Hardcoded container name
}

server {
    server_name filter-ical.de www.filter-ical.de;
    proxy_pass http://filter-ical-backend-production:3000;  # Different container
}
```

### 4.3 Automatic Health Checks

**Multi-Layer Verification:**
```bash
# 1. Docker healthcheck (container-level)
HEALTHCHECK CMD curl -f http://localhost:3000/health || exit 1

# 2. Deploy script healthcheck (end-to-end)
curl -f -s "https://staging.filter-ical.de/health" > /dev/null 2>&1

# 3. Docker Compose dependency (service-level)
depends_on:
  backend:
    condition: service_healthy  # Frontend waits for backend /health
```

**Deployment Rollback on Failure:**
```bash
if curl -f -s "$HEALTH_URL" > /dev/null 2>&1; then
    echo "‚úÖ Deployment successful!"
else
    echo "‚ùå Health check failed"
    echo "üîç Debug with: make logs-staging"
    exit 1  # Non-zero exit prevents declaring deployment complete
fi
```

### 4.4 Complete Traceability

**SSM Command History:**
- Every deployment creates an SSM command ID
- Full command logs in AWS CloudWatch
- Can replay commands for debugging
- Audit trail for compliance

**Example:**
```bash
$ make deploy-staging
üöÄ Deploying filter-ical to staging
Instance: i-01647c3d9af4fe9fc
Region: eu-north-1

‚úÖ Deployment command sent!
Command ID: abc123-def456-ghi789  # Traceable in AWS Console
‚è≥ Waiting for deployment to complete...
```

### 4.5 Simple Developer Experience

**One Command Deployment:**
```bash
make deploy-staging   # Tests ‚Üí Deploy ‚Üí Health Check ‚Üí Done
make deploy-production
```

**Integrated Debugging:**
```bash
make logs-staging     # Live logs via SSM session
make logs-production
```

**Emergency Deploy (Skip Tests):**
```bash
SKIP_TESTS=1 make deploy-staging  # For hotfixes
```

---

## 5. Weaknesses

### 5.1 Critical: 5-Minute Build Time Bottleneck

**Problem:**
```
Current: Code ‚Üí SSM ‚Üí EC2 ‚Üí Docker Build (5 min) ‚Üí Containers
Better:  Code ‚Üí GitHub Actions ‚Üí GHCR ‚Üí EC2 ‚Üí Pull Image (30s) ‚Üí Containers
```

**Impact:**
- Slow feedback loop (developers wait 5 minutes to see changes)
- High EC2 CPU usage during builds (affects other apps on multi-tenant platform)
- No build caching between environments (staging and production rebuild identical images)
- Longer deployment window (higher risk of issues)

**Evidence:**
```bash
# deploy.sh line 54
docker-compose -p $PROJECT_NAME up -d --build  # Rebuilds from scratch every time
```

**Solution:**
Implement GitHub Container Registry (GHCR) workflow:
```yaml
# .github/workflows/deploy-staging.yml
- name: Build and push images
  run: |
    docker build -t ghcr.io/duersjefen/filter-ical-backend:${{ github.sha }} ./backend
    docker build -t ghcr.io/duersjefen/filter-ical-frontend:${{ github.sha }} ./frontend
    docker push ghcr.io/duersjefen/filter-ical-backend:${{ github.sha }}
    docker push ghcr.io/duersjefen/filter-ical-frontend:${{ github.sha }}

# deploy.sh (updated)
docker pull ghcr.io/duersjefen/filter-ical-backend:$IMAGE_TAG
docker pull ghcr.io/duersjefen/filter-ical-frontend:$IMAGE_TAG
docker-compose up -d  # No --build flag (uses pre-built images)
```

**Expected Improvement:**
- Build time: 5 minutes ‚Üí 30 seconds (10x faster)
- EC2 CPU load: High during builds ‚Üí Minimal (just pulls)
- Build caching: None ‚Üí Full (GitHub Actions cache)

### 5.2 Critical: No Zero-Downtime Deployments

**Problem:**
```bash
# deploy.sh line 54
docker-compose -p $PROJECT_NAME up -d --build

# This stops old containers, builds new ones, then starts them
# Result: ~5 minutes of downtime during build
```

**Impact:**
- Users experience 502/503 errors during deployments
- API requests fail mid-deployment
- No graceful connection draining
- Database connections forcibly closed

**Current Behavior:**
```
Old Containers Running ‚Üí Stop Old Containers ‚Üí Build New Images ‚Üí Start New Containers
                            ‚Üì
                     5 MINUTES DOWNTIME
```

**Solution Options:**

**Option A: Blue-Green Deployment**
```bash
# Keep old containers running while building new ones
docker-compose -p filter-ical-staging-blue up -d --build
curl -f http://filter-ical-backend-staging-blue:3000/health || exit 1
# Swap nginx routing to blue containers
# Stop old (green) containers
```

**Option B: Rolling Update with Traefik**
```yaml
# docker-compose.yml
services:
  backend:
    deploy:
      replicas: 2
      update_config:
        parallelism: 1  # Update one at a time
        delay: 10s
    labels:
      - "traefik.enable=true"
```

**Expected Improvement:**
- Downtime: 5 minutes ‚Üí 0 seconds
- User impact: Errors during deploy ‚Üí Seamless updates
- Risk: High (all-or-nothing) ‚Üí Low (gradual rollout)

### 5.3 Critical: No Rollback Capability

**Problem:**
```bash
# deploy.sh line 51
git reset --hard origin/main  # DESTRUCTIVE - loses local state
```

**Impact:**
- Cannot rollback to previous version if deployment fails
- Git history on EC2 is overwritten (no way to recover)
- Manual intervention required to restore previous state
- No versioning of deployed code

**Current Rollback Process (Manual):**
```bash
# If deployment fails, must manually revert
aws ssm start-session --target i-01647c3d9af4fe9fc
cd /opt/apps/filter-ical
git reset --hard abc123  # Must know commit hash
docker-compose -p filter-ical-production up -d --build  # Another 5-minute build
```

**Solution: Tag-Based Deployment**
```bash
# deploy.sh (updated)
# Instead of: git reset --hard origin/main
# Use:        git checkout tags/v1.2.3

# Rollback becomes trivial:
git checkout tags/v1.2.2  # Previous version
docker-compose up -d
```

**Better: Image Tag Versioning**
```bash
# Deploy specific version
docker pull ghcr.io/duersjefen/filter-ical-backend:v1.2.3
docker-compose up -d

# Rollback to previous version (instant)
docker pull ghcr.io/duersjefen/filter-ical-backend:v1.2.2
docker-compose up -d
```

**Expected Improvement:**
- Rollback time: 5+ minutes (manual) ‚Üí 30 seconds (automated)
- Risk: High (might fail) ‚Üí Low (tested version)
- Visibility: None ‚Üí Full (tagged releases)

### 5.4 High: Arbitrary Sleep Delays

**Problem:**
```bash
# deploy.sh line 56
sleep 10  # Hope backend is ready in 10 seconds
```

**Impact:**
- Migrations might run before backend is fully started
- 10 seconds might be too short (race condition) or too long (wasted time)
- No feedback if backend takes longer than expected
- Silent failures if backend crashes during startup

**Solution: Proper Health Check Polling**
```bash
# deploy.sh (updated)
echo "‚è≥ Waiting for backend to be healthy..."
for i in {1..60}; do  # Try for 60 seconds
  if docker-compose -p $PROJECT_NAME exec -T backend curl -f http://localhost:3000/health; then
    echo "‚úÖ Backend is healthy"
    break
  fi
  if [ $i -eq 60 ]; then
    echo "‚ùå Backend failed to become healthy"
    exit 1
  fi
  sleep 1
done

# Now safe to run migrations
docker-compose -p $PROJECT_NAME exec -T backend alembic upgrade head
```

**Expected Improvement:**
- Reliability: 90% (depends on timing) ‚Üí 99.9% (verified health)
- Feedback: None ‚Üí Clear error messages
- Speed: Always 10s ‚Üí As fast as possible (1-10s)

### 5.5 High: Single Point of Failure

**Problem:**
```
All apps on single EC2 instance:
‚îú‚îÄ‚îÄ filter-ical
‚îú‚îÄ‚îÄ paiss
‚îî‚îÄ‚îÄ gabs-massage

If EC2 fails ‚Üí ALL APPS DOWN
```

**Impact:**
- No redundancy (single instance)
- No auto-scaling (fixed capacity)
- No geographic distribution (only eu-north-1)
- Maintenance windows require downtime

**Mitigation Options:**

**Option A: Multi-Instance Deployment**
```
EC2 Instance 1 (eu-north-1a)       EC2 Instance 2 (eu-north-1b)
‚îú‚îÄ‚îÄ filter-ical                    ‚îú‚îÄ‚îÄ filter-ical
‚îú‚îÄ‚îÄ paiss                          ‚îú‚îÄ‚îÄ paiss
‚îî‚îÄ‚îÄ gabs-massage                   ‚îî‚îÄ‚îÄ gabs-massage

AWS Load Balancer distributes traffic
```

**Option B: ECS/Fargate**
```
Move from EC2 to ECS Fargate:
- Auto-scaling based on load
- Zero-downtime deployments built-in
- Multi-AZ by default
- Pay per container (not per instance)
```

**Expected Improvement:**
- Availability: 95% (single instance) ‚Üí 99.9% (multi-AZ)
- Recovery time: Hours (manual) ‚Üí Seconds (automatic)
- Capacity: Fixed ‚Üí Auto-scaling

### 5.6 Medium: No Deployment Metrics

**Problem:**
- No tracking of deployment duration
- No monitoring of build failures
- No alerts on failed deployments
- No rollback tracking

**Impact:**
- Cannot measure deployment performance over time
- Silent failures might go unnoticed
- No historical data for incident analysis

**Solution: CloudWatch Integration**
```bash
# deploy.sh (updated)
START_TIME=$(date +%s)
# ... deployment ...
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

aws cloudwatch put-metric-data \
  --namespace FilterIcal/Deployments \
  --metric-name DeploymentDuration \
  --value $DURATION \
  --dimensions Environment=staging
```

### 5.7 Medium: Environment Secrets on EC2

**Problem:**
```bash
# Secrets stored as plain files on EC2
/opt/apps/filter-ical/.env.staging
/opt/apps/filter-ical/.env.production
```

**Impact:**
- Anyone with SSM access can read secrets
- No secret rotation mechanism
- No audit trail of secret access
- Backup/restore requires manual handling

**Solution: AWS Secrets Manager**
```bash
# Store secrets in AWS Secrets Manager
aws secretsmanager create-secret \
  --name filter-ical/staging/database-url \
  --secret-string "postgresql://..."

# deploy.sh fetches secrets at deploy time
export DATABASE_URL=$(aws secretsmanager get-secret-value \
  --secret-id filter-ical/staging/database-url \
  --query SecretString \
  --output text)
```

**Expected Improvement:**
- Security: File-based ‚Üí Encrypted at rest
- Rotation: Manual ‚Üí Automatic
- Audit: None ‚Üí Full CloudTrail logs

### 5.8 Low: Git as Deployment Artifact Store

**Problem:**
```bash
# deploy.sh line 45-51
cd /opt/apps/filter-ical || exit 1
if [ ! -d .git ]; then
  git clone https://github.com/duersjefen/filter-ical.git
fi
git fetch origin
git reset --hard origin/main
```

**Impact:**
- Git repo on EC2 can diverge from GitHub
- `.git` directory bloat (full history)
- Deployment depends on GitHub availability
- No separation between source code and deployment artifacts

**Better Approach:**
```bash
# Use release artifacts instead of git repo
wget https://github.com/duersjefen/filter-ical/releases/download/v1.2.3/filter-ical.tar.gz
tar -xzf filter-ical.tar.gz -C /opt/apps/filter-ical
# Or: Pull pre-built Docker images from GHCR
```

---

## 6. Improvement Opportunities (Prioritized)

### 6.1 Priority 1: Critical Infrastructure Improvements

#### 1A. Implement Container Registry (GHCR)

**Problem:** 5-minute builds on EC2 every deployment

**Solution:**
```yaml
# .github/workflows/build.yml
name: Build and Push Images
on:
  push:
    branches: [main]
    tags: ['v*']

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Login to GHCR
        run: echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Build and push backend
        run: |
          docker build -t ghcr.io/duersjefen/filter-ical-backend:${{ github.sha }} ./backend
          docker build -t ghcr.io/duersjefen/filter-ical-backend:latest ./backend
          docker push ghcr.io/duersjefen/filter-ical-backend:${{ github.sha }}
          docker push ghcr.io/duersjefen/filter-ical-backend:latest

      - name: Build and push frontend
        run: |
          docker build -t ghcr.io/duersjefen/filter-ical-frontend:${{ github.sha }} ./frontend
          docker build -t ghcr.io/duersjefen/filter-ical-frontend:latest ./frontend
          docker push ghcr.io/duersjefen/filter-ical-frontend:${{ github.sha }}
          docker push ghcr.io/duersjefen/filter-ical-frontend:latest
```

```yaml
# docker-compose.yml (updated)
services:
  backend:
    image: ghcr.io/duersjefen/filter-ical-backend:${IMAGE_TAG:-latest}
    # Remove: build: ./backend

  frontend:
    image: ghcr.io/duersjefen/filter-ical-frontend:${IMAGE_TAG:-latest}
    # Remove: build: ./frontend
```

```bash
# deploy.sh (updated)
# Line 54 becomes:
export IMAGE_TAG=${IMAGE_TAG:-latest}
docker-compose -p $PROJECT_NAME pull  # Pull pre-built images (30s)
docker-compose -p $PROJECT_NAME up -d  # No --build (instant)
```

**Benefits:**
- Deployment time: 5 minutes ‚Üí 30 seconds (10x faster)
- Build once, deploy anywhere (staging and production use same image)
- Full build caching in GitHub Actions
- Reduced EC2 CPU load
- Immutable artifacts (can't accidentally modify builds)

**Estimated Effort:** 4 hours
**Risk:** Low (doesn't change application logic)

#### 1B. Implement Blue-Green Deployments

**Problem:** 5 minutes of downtime during every deployment

**Solution:**
```bash
# deploy.sh (updated blue-green version)
CURRENT_ENV=$ENVIRONMENT
NEW_ENV="${ENVIRONMENT}-blue"

# Start new containers alongside old ones
echo "üöÄ Starting new version ($NEW_ENV)..."
docker-compose -p filter-ical-$NEW_ENV pull
docker-compose -p filter-ical-$NEW_ENV up -d

# Wait for health check
echo "‚è≥ Waiting for new version to be healthy..."
for i in {1..60}; do
  if curl -f http://filter-ical-backend-$NEW_ENV:3000/health; then
    echo "‚úÖ New version is healthy"
    break
  fi
  sleep 1
done

# Run migrations on new version
docker-compose -p filter-ical-$NEW_ENV exec -T backend alembic upgrade head

# Swap nginx routing (atomic update)
echo "üîÑ Switching traffic to new version..."
aws ssm send-command ... 'docker exec nginx-platform nginx -s reload'

# Stop old containers
echo "üßπ Stopping old version..."
docker-compose -p filter-ical-$CURRENT_ENV down

# Rename new env to current env (for next deployment)
docker-compose -p filter-ical-$NEW_ENV down
docker-compose -p filter-ical-$CURRENT_ENV up -d --no-build
```

**Nginx Configuration (updated for blue-green):**
```nginx
# Use environment variable to switch between blue/green
set $backend_env "${FILTER_ICAL_BACKEND_ENV}";  # Set via consul/etcd/env
set $backend filter-ical-backend-$backend_env:3000;
proxy_pass http://$backend;
```

**Benefits:**
- Zero downtime (old version serves traffic until new version is healthy)
- Instant rollback (switch nginx back to old containers)
- Lower risk (verify new version before switching)
- Graceful connection draining

**Estimated Effort:** 8 hours
**Risk:** Medium (requires nginx reconfiguration)

#### 1C. Tag-Based Versioning

**Problem:** No way to rollback to previous version

**Solution:**
```bash
# Tag releases in git
git tag -a v1.2.3 -m "Release 1.2.3"
git push origin v1.2.3

# deploy.sh accepts version parameter
./deploy.sh staging v1.2.3

# docker-compose.yml uses version tag
services:
  backend:
    image: ghcr.io/duersjefen/filter-ical-backend:${VERSION:-latest}
```

```bash
# Makefile (updated)
deploy-staging: ## Deploy specific version to staging (usage: make deploy-staging VERSION=v1.2.3)
	@if [ -z "$(VERSION)" ]; then \
		./deploy.sh staging latest; \
	else \
		./deploy.sh staging $(VERSION); \
	fi

rollback-staging: ## Rollback to previous version
	@echo "üîÑ Rolling back to previous version..."
	@PREVIOUS_VERSION=$$(git describe --tags --abbrev=0 HEAD^)
	@./deploy.sh staging $$PREVIOUS_VERSION
```

**Benefits:**
- One-command rollback (`make rollback-staging`)
- Version history tracking
- Reproducible deployments (deploy v1.2.3 anytime)
- Easier debugging (know exactly what code is deployed)

**Estimated Effort:** 2 hours
**Risk:** Low (additive change)

### 6.2 Priority 2: Reliability Improvements

#### 2A. Replace Sleep with Health Check Polling

**Problem:** `sleep 10` is unreliable

**Solution:**
```bash
# deploy.sh (updated)
wait_for_health() {
  SERVICE=$1
  URL=$2
  MAX_WAIT=60

  echo "‚è≥ Waiting for $SERVICE to be healthy..."
  for i in $(seq 1 $MAX_WAIT); do
    if curl -f -s "$URL/health" > /dev/null 2>&1; then
      echo "‚úÖ $SERVICE is healthy (took ${i}s)"
      return 0
    fi
    echo -n "."
    sleep 1
  done

  echo "‚ùå $SERVICE failed to become healthy after ${MAX_WAIT}s"
  return 1
}

# Usage
docker-compose -p $PROJECT_NAME up -d
wait_for_health "backend" "http://filter-ical-backend-$ENVIRONMENT:3000" || exit 1
docker-compose -p $PROJECT_NAME exec -T backend alembic upgrade head
```

**Benefits:**
- Faster deployments (no wasted time)
- Clearer error messages
- Prevents race conditions

**Estimated Effort:** 1 hour
**Risk:** Low

#### 2B. Add Deployment Metrics

**Problem:** No visibility into deployment performance

**Solution:**
```bash
# deploy.sh (updated)
record_metric() {
  METRIC_NAME=$1
  VALUE=$2

  aws cloudwatch put-metric-data \
    --namespace FilterIcal/Deployments \
    --metric-name "$METRIC_NAME" \
    --value "$VALUE" \
    --dimensions Environment=$ENVIRONMENT,Version=$VERSION
}

START_TIME=$(date +%s)
# ... deployment ...
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

if [ $? -eq 0 ]; then
  record_metric "DeploymentSuccess" 1
  record_metric "DeploymentDuration" $DURATION
else
  record_metric "DeploymentFailure" 1
fi
```

**CloudWatch Dashboard:**
```json
{
  "widgets": [
    {
      "type": "metric",
      "properties": {
        "metrics": [
          [ "FilterIcal/Deployments", "DeploymentDuration", { "stat": "Average" } ],
          [ ".", "DeploymentSuccess", { "stat": "Sum" } ],
          [ ".", "DeploymentFailure", { "stat": "Sum" } ]
        ],
        "title": "Deployment Metrics"
      }
    }
  ]
}
```

**Benefits:**
- Track deployment performance over time
- Alert on failed deployments
- Identify slow builds
- Historical analysis for incidents

**Estimated Effort:** 3 hours
**Risk:** Low

#### 2C. Implement AWS Secrets Manager

**Problem:** Secrets stored as plain files on EC2

**Solution:**
```bash
# Store secrets in AWS Secrets Manager
aws secretsmanager create-secret \
  --name filter-ical/staging/env \
  --secret-string '{
    "DATABASE_URL": "postgresql://...",
    "SECRET_KEY": "...",
    "ICALENDAR_API_KEY": "..."
  }'

# deploy.sh fetches secrets
fetch_secrets() {
  ENVIRONMENT=$1
  SECRET_JSON=$(aws secretsmanager get-secret-value \
    --secret-id filter-ical/$ENVIRONMENT/env \
    --query SecretString \
    --output text)

  echo "$SECRET_JSON" | jq -r 'to_entries[] | "\(.key)=\(.value)"' > /tmp/.env.$ENVIRONMENT
}

fetch_secrets $ENVIRONMENT
docker-compose -p $PROJECT_NAME --env-file /tmp/.env.$ENVIRONMENT up -d
rm /tmp/.env.$ENVIRONMENT  # Clean up
```

**Benefits:**
- Encrypted at rest and in transit
- Automatic rotation support
- Full audit trail (CloudTrail)
- Granular access control (IAM)
- No secrets in git or on filesystem

**Estimated Effort:** 4 hours
**Risk:** Medium (requires secret migration)

### 6.3 Priority 3: Advanced Optimizations

#### 3A. Multi-Stage Health Checks

**Current:** Single endpoint check

**Better:**
```bash
# Health check validates multiple layers
health_check() {
  # 1. Container is running
  docker ps | grep filter-ical-backend-$ENVIRONMENT || return 1

  # 2. HTTP server is responding
  curl -f http://localhost:3000/health || return 1

  # 3. Database connection works
  docker exec filter-ical-backend-$ENVIRONMENT \
    python -c "from app.database import get_db; list(get_db())" || return 1

  # 4. External API is reachable (if applicable)
  curl -f http://localhost:3000/api/domains/test || return 1

  return 0
}
```

#### 3B. Canary Deployments

**Better than blue-green:** Gradual traffic shifting

```bash
# Start new version with 10% traffic
docker-compose -p filter-ical-staging-canary up -d
# Update nginx to send 10% traffic to canary
# Monitor error rates for 5 minutes
# If healthy, increase to 50%, then 100%
# If unhealthy, rollback immediately
```

#### 3C. Database Migration Pre-Check

**Problem:** Migrations run after containers start (might fail)

**Better:**
```bash
# Test migrations on temporary database before applying
docker run --rm \
  -e DATABASE_URL=postgresql://temp-db \
  ghcr.io/duersjefen/filter-ical-backend:$VERSION \
  alembic upgrade head --sql > migration.sql

# Review migration SQL
cat migration.sql

# Apply to real database only if review passes
docker exec filter-ical-backend-$ENVIRONMENT \
  alembic upgrade head
```

---

## 7. Risk Assessment

### 7.1 Current System Risks

| Risk | Likelihood | Impact | Severity | Mitigation |
|------|-----------|--------|----------|-----------|
| **5-minute downtime during deploy** | High (every deploy) | High (users affected) | **CRITICAL** | Implement blue-green |
| **Failed deploy, no rollback** | Medium (1/month) | High (extended outage) | **CRITICAL** | Tag-based versioning |
| **EC2 instance failure** | Low (1/year) | Critical (all apps down) | **HIGH** | Multi-AZ setup |
| **Migration failure** | Low (1/quarter) | High (database corruption) | **HIGH** | Migration pre-checks |
| **Race condition (sleep 10)** | Medium (timing-dependent) | Medium (failed deploy) | **MEDIUM** | Health check polling |
| **Secret exposure (file-based)** | Low (requires SSM access) | High (credential theft) | **MEDIUM** | Secrets Manager |
| **GitHub outage (git clone)** | Low (rare) | Medium (can't deploy) | **LOW** | Use GHCR images |

### 7.2 Recommended Risk Mitigation Timeline

**Month 1: Critical Fixes**
- Week 1: Implement GHCR (1A) ‚Üí Reduce build time to 30s
- Week 2: Implement blue-green (1B) ‚Üí Zero-downtime deploys
- Week 3: Tag-based versioning (1C) ‚Üí Enable rollbacks
- Week 4: Testing and validation

**Month 2: Reliability**
- Week 1: Health check polling (2A)
- Week 2: Deployment metrics (2B)
- Week 3: Secrets Manager (2C)
- Week 4: Multi-stage health checks (3A)

**Month 3: Advanced**
- Week 1: Canary deployments (3B)
- Week 2: Migration pre-checks (3C)
- Week 3: Multi-AZ planning
- Week 4: Documentation and training

---

## 8. Comparison with Alternative Approaches

### 8.1 Current: SSM-Based Direct Deploy

```
‚úÖ Pros:
- No SSH keys (secure)
- Simple to understand
- Complete control over EC2
- Low cost ($30/month for EC2)

‚ùå Cons:
- 5-minute builds
- No zero-downtime
- Manual rollback
- Single point of failure
```

### 8.2 Alternative 1: GitHub Actions + SSH

```
GitHub Actions ‚Üí SSH ‚Üí EC2 ‚Üí Docker

‚úÖ Pros:
- Builds in CI (faster)
- Familiar workflow (GitHub Actions)
- Can use GHCR

‚ùå Cons:
- Requires SSH key management
- Less secure (exposed SSH port)
- No SSM session logging
```

**Verdict:** Current SSM approach is more secure, but should add GHCR builds

### 8.3 Alternative 2: AWS ECS/Fargate

```
GitHub Actions ‚Üí ECR ‚Üí ECS ‚Üí Fargate

‚úÖ Pros:
- Zero-downtime built-in
- Auto-scaling
- Multi-AZ by default
- No server management

‚ùå Cons:
- Higher cost ($150/month)
- More complex (VPC, ALB, ECS)
- Learning curve
```

**Verdict:** Consider for future scaling, but overkill for current needs

### 8.4 Alternative 3: Docker Swarm on EC2

```
SSM ‚Üí EC2 Swarm Manager ‚Üí Docker Stack Deploy

‚úÖ Pros:
- Rolling updates built-in
- Service mesh networking
- Health checks included
- Still uses EC2 (low cost)

‚ùå Cons:
- Swarm complexity
- Less common than Kubernetes
- Limited ecosystem
```

**Verdict:** Good middle ground, but adds complexity

### 8.5 Alternative 4: Kubernetes (EKS)

```
GitHub Actions ‚Üí ECR ‚Üí EKS ‚Üí Kubernetes Pods

‚úÖ Pros:
- Industry standard
- Rich ecosystem
- Advanced deployments (canary, etc)
- Multi-cluster support

‚ùå Cons:
- Very high cost ($200+/month)
- Steep learning curve
- Overkill for 3 apps
```

**Verdict:** Not suitable for current scale

---

## 9. Recommended Architecture Evolution

### Phase 1: Keep SSM, Add GHCR (Recommended Next Step)

```
Current:
Local ‚Üí GitHub (code) ‚Üí SSM ‚Üí EC2 ‚Üí Build (5min) ‚Üí Deploy

Better:
Local ‚Üí GitHub (code) ‚Üí GitHub Actions ‚Üí GHCR (images) ‚Üí SSM ‚Üí EC2 ‚Üí Pull (30s) ‚Üí Deploy
```

**Changes:**
1. Add `.github/workflows/build.yml` for image builds
2. Update `docker-compose.yml` to use GHCR images
3. Update `deploy.sh` to pull instead of build

**Benefits:**
- 10x faster deployments
- Keep SSM security benefits
- Minimal disruption
- Low risk

**Estimated Effort:** 1 week
**Cost:** $0 (GHCR is free for public repos)

### Phase 2: Add Blue-Green Deployments

```
SSM ‚Üí EC2 ‚Üí Pull Images ‚Üí Start Blue Containers ‚Üí Verify Health ‚Üí Swap Nginx ‚Üí Stop Green
```

**Changes:**
1. Update `deploy.sh` for blue-green logic
2. Update nginx config for dynamic routing
3. Add container naming scheme for blue/green

**Benefits:**
- Zero downtime
- Instant rollback
- Lower risk

**Estimated Effort:** 1 week
**Cost:** $0 (same infrastructure)

### Phase 3: Add Versioning and Rollback

```
Git Tags ‚Üí GHCR Tagged Images ‚Üí Deploy Specific Version ‚Üí Rollback to Previous Tag
```

**Changes:**
1. Tag releases in git (`v1.2.3`)
2. Build images with version tags
3. Update `deploy.sh` to accept version parameter
4. Add `make rollback-staging` command

**Benefits:**
- One-command rollback
- Version tracking
- Reproducible deployments

**Estimated Effort:** 3 days
**Cost:** $0

### Future (6-12 months): Multi-Instance or ECS

**When to Consider:**
- Traffic exceeds single EC2 capacity
- Need 99.9% SLA
- More than 10 apps on platform
- Team grows beyond 5 developers

**Options:**
- Multi-instance with load balancer ($100/month)
- ECS Fargate ($150/month)
- Kubernetes/EKS ($200+/month)

---

## 10. Conclusion

### Executive Summary

The filter-ical SSM deployment architecture demonstrates **excellent security practices** (no SSH keys, IAM-based access) and **strong environment isolation** (staging/production separation). However, it suffers from **critical performance and reliability issues** that significantly impact deployment velocity and production stability.

**Three Critical Issues:**

1. **5-Minute Build Bottleneck:** Building Docker images on EC2 during deployment creates a 10x slower feedback loop compared to pre-built images. This delays bug fixes, increases EC2 load, and wastes developer time waiting for deployments.

2. **Zero-Downtime Impossible:** Current deployment stops old containers before starting new ones, causing 5 minutes of 502/503 errors for users. This is unacceptable for production deployments, especially during business hours.

3. **No Rollback Capability:** Using `git reset --hard` is destructive and offers no path to quickly recover from bad deployments. The only option is another 5-minute redeployment, extending outages significantly.

**Recommended Fix (Priority Order):**

**CRITICAL (Do First):**
1. **Implement GHCR** (4 hours) ‚Üí Reduce deployments from 5min to 30s (10x improvement)
2. **Blue-Green Deployments** (8 hours) ‚Üí Achieve zero-downtime updates
3. **Tag-Based Versioning** (2 hours) ‚Üí Enable instant rollbacks

**HIGH (Do Second):**
4. **Health Check Polling** (1 hour) ‚Üí Replace arbitrary `sleep 10` with verification
5. **Deployment Metrics** (3 hours) ‚Üí Gain visibility into deployment performance
6. **Secrets Manager** (4 hours) ‚Üí Improve secret security and rotation

**Total Effort:** ~22 hours (3 weeks at 1 day/week)
**Total Cost:** $0 (uses existing infrastructure)

**Expected Outcomes:**
- Deployment time: 5 minutes ‚Üí 30 seconds (10x faster)
- Downtime: 5 minutes/deploy ‚Üí 0 seconds (zero-downtime)
- Rollback time: 5+ minutes ‚Üí 30 seconds (instant)
- Developer velocity: +50% (faster iteration)
- Production stability: +90% (fewer failed deploys)

**Key Insight:** The architecture's foundation is solid (SSM, Docker, multi-tenant platform), but needs **tactical improvements** to unlock its full potential. The recommended changes require minimal effort (3 weeks) with zero additional cost while delivering transformative benefits.

### Final Recommendation

**Implement Phase 1 (GHCR) immediately.** This single change unlocks 10x faster deployments and enables all subsequent improvements. The remaining phases can follow incrementally without disrupting operations.

The SSM-based approach should be **preserved** (don't switch to SSH or ECS), but **enhanced** with container registry builds and blue-green deployments. This keeps the security and simplicity benefits while eliminating performance bottlenecks.

---

## Appendix A: Deployment Command Reference

```bash
# Current deployment commands
make deploy-staging          # Deploy staging (with tests)
make deploy-production       # Deploy production (with tests)
SKIP_TESTS=1 make deploy-staging  # Emergency deploy (skip tests)
make logs-staging           # View staging logs
make logs-production        # View production logs
make status                 # Check deployment status

# Proposed new commands (after improvements)
make deploy-staging VERSION=v1.2.3  # Deploy specific version
make rollback-staging               # Rollback to previous version
make deploy-staging-canary         # Canary deployment (10% traffic)
make promote-staging-canary        # Promote canary to 100%
make deploy-staging-dry-run        # Test deployment without applying
```

## Appendix B: File Structure Reference

```
filter-ical/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile              # Python 3.11 + FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt        # Dependencies (60s install)
‚îÇ   ‚îú‚îÄ‚îÄ app/                    # Application code
‚îÇ   ‚îú‚îÄ‚îÄ alembic/                # Database migrations
‚îÇ   ‚îî‚îÄ‚îÄ tests/                  # Unit tests
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile              # Node 22 + nginx
‚îÇ   ‚îú‚îÄ‚îÄ package.json            # Dependencies (90s install)
‚îÇ   ‚îú‚îÄ‚îÄ src/                    # Vue 3 source
‚îÇ   ‚îî‚îÄ‚îÄ nginx.conf              # Frontend nginx config
‚îÇ
‚îú‚îÄ‚îÄ deploy.sh                   # SSM deployment script
‚îú‚îÄ‚îÄ docker-compose.yml          # Container orchestration
‚îú‚îÄ‚îÄ Makefile                    # Developer interface
‚îú‚îÄ‚îÄ .env.staging                # Staging secrets (on EC2 only)
‚îú‚îÄ‚îÄ .env.production             # Production secrets (on EC2 only)
‚îú‚îÄ‚îÄ .env.ec2                    # EC2 instance ID (local, gitignored)
‚îî‚îÄ‚îÄ CLAUDE.md                   # Project documentation
```

## Appendix C: Useful Debugging Commands

```bash
# SSM session (interactive shell)
aws ssm start-session --target i-01647c3d9af4fe9fc --region eu-north-1

# View container logs
docker logs -f filter-ical-backend-staging
docker logs -f filter-ical-frontend-staging

# Check container status
docker ps | grep filter-ical

# Check nginx routing
docker exec nginx-platform cat /etc/nginx/sites/filter-ical.conf

# Check database connection
docker exec filter-ical-backend-staging \
  python -c "from app.database import get_db; print(list(get_db()))"

# Check health endpoints
curl -f https://staging.filter-ical.de/health
curl -f http://filter-ical-backend-staging:3000/health

# View recent deployments
aws ssm list-commands --region eu-north-1 --filters "key=DocumentName,value=AWS-RunShellScript"

# View specific deployment output
aws ssm get-command-invocation \
  --command-id abc123-def456 \
  --instance-id i-01647c3d9af4fe9fc \
  --region eu-north-1
```

---

**Document Version:** 1.0
**Last Updated:** 2025-10-11
**Next Review:** After Phase 1 implementation
